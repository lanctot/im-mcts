
We thank the reviewers for their valuable feedback. We are happy to see the interested generated by implicit minimax backups. 

We hope to address the many interesting points in a comprehensive (journal) paper. For this first six-page conference paper, we chose to focus on a thorough evaluation across several domains and show the promise for improving practical performance. We intentionally traded depth of analysis in one domain for breadth across several domains, which resulted in general observations and patterns such as the relationship between alpha and performance gain (and the effect as a function of the amount/level of domain knowledge.)

If accepted, we will clarify/improve the "Empirical Evaluation"+"Conclusion" sections, and release the MCTS framework used for Breakthrough+Kalah open-source.

During the mid-game, searches in Breakthrough average 12000-30000 simulations/second, depending on the enhancements used. In Kalah, it's 95000. In LOA, it's 16000 using the simple playouts and 4000 using alpha-beta playouts. 

Regarding the comparison to minimax, LOA is well-known and has been extensively researched. It is especially relevant given previous work comparing MCTS/minimax. Experiments were performed in MC-LOA, a world-champion engine winning the latest Olympiad. The benchmark minimax player is MIA, the world-best minimax-player upon which MC-LOA is based, winning the previous 4 Olympiads. Both use state-of-the-art techniques, including highly-optimized alpha-beta playouts as well as a sophisticated evaluation function based on 14 years of research. MIA includes enhancements: static and dynamic move ordering, iterative deepening, killer moves, history heuristic, enhanced transposition table cutoffs, null-move pruning, multi-cut, realization probability search, quiescence search, and negascout/PVS. 

We tested different evaluation functions in Breakthrough: a simple one, and a sophisticated one taken from a state-of-the-art engine (see "Empirical Evaluation".) Both are computed incrementally adding <1% overhead. 

Performance increases significantly even when using simple piece count differences, showing advantages with weak heuristic information which is often readily available. The ranges of heuristic values are [-1,1] (see "Empirical Evaluation".)

Terminal states are indeed considered perfect values (proven wins/losses/draws). These are back-propagated by MCTS-Solver (see Winands et al. 2008a, also explained in "Related Work") which override both heuristic values and Monte Carlo estimates. 

Regarding baselines in Breakthrough: we used tournaments on thousands of games to determine the best combination, with details in the appendix (included in a technical report if accepted). Using the default playout policy, MCTS wins 94.3% of games against MCTS with uniform random playouts. Our best baseline MCTS(ege0.1,det0.5) beats MCTS using our default playout policy in 68.8% of games.  

To the best of our understanding, "virtual playouts" in the cited paper is equivalent to node priors, which we compare to in Breakthrough. 

While Kalah is relatively less explored, we believe it nonetheless helps characterize the type of domain MCTS(im\alpha) will work in and complements relevant previous MCTS/minimax comparisons (Ramanujan & Selman 2011). It also produces a consistent alpha-vs-performance pattern.

Explaining the precise cause/source of the improvement requires further work. However, this paper shows that the approach leads to clear performance gains compared to state-of-the-art techniques, with recurring patters across three domains under varying experimental configurations. 


