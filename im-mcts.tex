%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{nopageno}
\usepackage{url}

\usepackage[algo2e, noend, noline, linesnumbered]{algorithm2e}
\providecommand{\SetAlgoLined}{\SetLine}  
\providecommand{\DontPrintSemicolon}{\dontprintsemicolon}
\DontPrintSemicolon
\makeatletter
\newcommand{\pushline}{\Indp}% Indent
\newcommand{\popline}{\Indm}
\makeatother
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\captionsetup{compatibility=false}

%\pgfplotsset{compat=newest}
\usetikzlibrary{arrows,shapes,petri}

\newcommand{\bE}{\mathbb{E}}
\newcommand{\bQ}{\bar{Q}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\hQ}{\hat{Q}}
\newcommand{\hV}{\hat{V}}
\newcommand{\eg}{{\it e.g.,}~}
\newcommand{\ie}{{\it i.e.,}~}

\newcommand{\redbold}[1]{\textbf{\color{red}#1}} 
\newcommand{\markw}[1]{\textbf{\color{red} /* #1 (markw) */}} 
\newcommand{\marcl}[1]{\textbf{\color{red} /* #1 (marcl) */}} 

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Insert Your Title Here)
/Author (Put All Your Authors Here, Separated by Commas)}
\setcounter{secnumdepth}{0}  

\begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Monte Carlo Tree Search with Heuristic Evaluations\\using Implicit Minimax Backups}
\author{Authors}

\maketitle

\begin{abstract}
In this document we describe how minimax backups from evaluations functions can be combined implicitly 
with MCTS. 
\end{abstract}

\section{Introduction}

Monte Carlo Tree Search (MCTS)~\cite{mctssurvey,Coulom06Efficient,Kocsis06Bandit} is a ... 

\subsection{Related Work}

Several techniques for minimax-style backup rules in the simulation-based MCTS framework have already been proposed. 
The first was Coulom's original {\it maximum backpropagation}~\cite{Coulom06Efficient}. This method of backpropagation
suggests, after a number of simulations to a node has been reached, to switch to propagating the maximum value instead 
of the simulated (``average'') value. 
The rationale behind this choice is that after a certain point, the search algorithm should consider the node
{\it converged} and return an estimate of the best value. 
Maximum backpropagation focuses on estimating the value of the optimal action required for the parent nodes, which is a
``separate exploratory concern'' than simply identifying the best action at that node~\cite{Feldman13Theoretically}.
In games where minimax search works well, such as Mancala, modifying MCTS to 
use minimax-style backups and heuristic values instead of playouts was shown to be as powerful as a strong 
depth-first minimax search~\cite{Ramanujan11Tradeoffs}.
Similarly, there is further evidence suggesting not replacing the playout entirely, but terminating them early 
using heuristic evaluations, has increased the performance in Lines of Action (LOA)~\cite{Winands10MCTS-LOA}, 
Amazons~\cite{Kloetzer10Amazons,Lorentz08Amazons}, and Breakthrough~\cite{Lorentz13Breakthrough}. In LOA and Amazons, the 
MCTS players enhanced with evaluation functions outperform their minimax counterparts using the same evaluation function.

% mention score-bounded mcts
% mention hendrik's minimax methods
% use mcts-solver to motivate implicit minimax
%\marcl{Also mention MCTS + minimax hybrids (Baier \& Winands) and Score-bounded MCTS (...))}

One may want to combine minimax backups or searches without using an evaluation function. 
The prime example is MCTS-Solver~\cite{Winands08Solver}. When using 
MCTS-Solver, proven wins and losses are backpropagated as extra information in MCTS. When a node is proven to be a 
win or a loss, it no longer needs to be searched. This simple, domain-independent modification greatly enhances 
MCTS in many games (some examples). Score-bounded MCTS extends this idea to games with multiple outcomes, 
leading to $\alpha \beta$ style pruning in the tree~\cite{Cazenave10ScoreBounded}. Finally, one can do use hybrid
minimax searches in the tree to initialize nodes during expansion, enhance the playout, or to help MCTS-Solver 
in backpropagation~\cite{Baier13MinimaxHybrids}.

In this work, we propose to augment MCTS with the help of an implicitly-computed minimax search which uses
heuristic evaluations. In our case, these heuristic evaluations are not used to replace or terminate playouts, but 
rather as a {\it separate source of information} to guide the tree search during the selection phase. Like MCTS-Solver, 
the backpropagation of these values is treated differently and affect node selection. However, unlike MCTS-Solver and 
minimax hybrids, these modifications are based on heuristic evaluations rather than proven wins and losses. 
We show that (something good and insightful). 

\section{Background}

%The following notation is taken from~\cite{Sutton98}

A finite Markov Decision Process (MDP) is 4-tuple $(\cS, \cA, \cT, \cR)$. Here, $\cS$ is a finite non-empty set of {\it states}. 
$\cA$ is a finite non-empty set of {\it actions}, where we denote $\cA(s) \subseteq \cA$ the set of available actions at state $s$. 
$\cT : \cS \times \cA \mapsto \Delta \cS$ is a {\it transition function} mapping 
each state and action to a distribution over successor states. Finally, $\cR : \cS \times \cA \times \cS \mapsto \Re$ 
is a {\it reward function} mapping (state, action, successor state) triplets to numerical rewards. 

A two-player perfect information game is an augmented MDP with a specific form. 
Denote $\cZ \subset \cS$ the set of {\it terminal states}. 
In addition, for all nonterminal states $s' \in \cS - \cZ$, $\cR(s,a,s') = 0$. 
There is a {\it player identity function} $\tau : \cS - \cZ \mapsto \{1,2\}$. 
In this paper, we assume fully deterministic domains, so $\cT(s,a)$ maps to a single state, $s'$. However, the ideas proposed can 
be easily extended to domains with stochastic transitions. 

Monte Carlo Tree Search is a simulation-based best-first search algorithm that incrementally builds a model of the game 
$\cG$, in memory. Each search starts with from a {\it root state} $s_0 \in \cS - \cZ$, and initially sets $\cG = \emptyset$. 
Each simulation samples a trajectory $\rho = (s_0, a_0, s_1, a_1, \cdots, s_n)$, where $s_n \in \cZ$. 
The portion of the $\rho$ where $s_i \in \cG$ is called the {\it tree portion} and the remaining portion is
called the {\it playout portion}. In the tree portion, actions are chosen according to some {\it selection policy}. 
The first state encountered in the playout portion is {\it expanded} (added to $\cG$.) 
The actions chosen in the playout portion are determined by a specific {\it playout policy}. 
States $s \in \cG$ are referred to as {\it nodes} and statistics are  
maintained for each node $s$: the cumulative reward, $r_s$, and visit count, $n_s$. 
By popular convention, we define $r_{s,a} = r_{s'}$ where $s' = \cT(s,a)$, and similarly $n_{s,a} = n_{s'}$. 
Also, we use $r^{\tau}_s$ to denote the reward at state $s$ {\it with respect to player} $\tau(s)$. 
Finally, we denote  the set $C(s)$ to be the children of $s$ that are in $\cG$. 

Let $\hV(s,a)$ be an estimator for node $s$ and $\hQ(s,a)$ for the state-action pair. 
For example, one popular estimator is the observed mean over all simulations 
$\bQ(s,a) = r^{\tau}_{s,a} / n_{s,a}$. 
The most widely-used is based on a bandit algorithm called Upper Confidence Bounds (UCB)~\cite{Auer02Finite} in an algorithm adapted 
for MCTS called UCT~\cite{Kocsis06Bandit}, which selects action $a'$ using
\begin{equation}
\label{eq:select-ucb}
a' = \argmax_{a \in \cA(s)} \left\{ \hQ(s,a) + C \sqrt{\frac{\ln n_s}{n_{s,a}}} \right\}, 
\end{equation}
where $C$ is parameter determining the weight of exploration. 

\section{Implicit Minimax Backups in MCTS}

Now, suppose we are given an evaluation function $v_0(s)$ whose range is the same as that of the reward function $\cR$. 
Assuming $v_0(s)$ is a sensible indicator of the reward, we would like that this added source of information
strictly benefits MCTS. 
To make use of this information in MCTS, we add another value to maintain at each node, the 
{\it implicit minimax evaluation with respect to player} $\tau(s)$, $v^{\tau}_s$, and define $v^{\tau}_{s,a}$ as before. 
During backpropagation, $r_s$ and $n_s$ are updated in the usual way, and additionally $v^{\tau}_s$ is updated a minimax backup 
rule based on children values. Also, rather than using $\hQ = \bQ$ for selection in Equation~\ref{eq:select-ucb}, we use
\begin{equation}
\label{eq:imq}
\hQ^{\mathit{IM}}(s,a) = \frac{r^{\tau}_{s,a}}{n_{s,a}} + \alpha(n_s) v^{\tau}_{s,a}.
\end{equation}

Pseudo-code (without MCTS-Solver for now) is presented in Algorithm~\ref{alg}. There are three main changes from vanilla MCTS, two 
of which are on lines \ref{alg:imselect} and \ref{alg:imeval}. 
During selection, $\hQ^{\mathit{IM}}$ from Equation~\ref{eq:imq} replaces $\bQ$ in 
Equation~\ref{eq:select-ucb}. During backpropagation, the implicit minimax evaluations $v^{\tau}_s$ are updated based on 
the children's values. Note that a single $\max$ is used here since the evaluations are assumed to be in 
view of player $\tau(s)$.
%\footnote{Domain-dependent inversions or scalings may be necessary to convert the rewards. These details are 
%intentionally omitted from the pseudo-code for generality.}
The function $\alpha(n_s)$ will determine how much weight to attribute to these evaluations.  
Finally, during a node expansion on line~\ref{alg:imexpand}, the implicit minimax value is initialized to its heuristic
evaluation $v^{\tau}_s \leftarrow v^{\tau}_0(s)$. 

\newcommand{\MCTS}{{\sc MCTS}}
\newcommand{\Update}{{\sc Update}}
\newcommand{\Playout}{{\sc Playout}}
\newcommand{\Select}{{\sc Select}}
\newcommand{\Expand}{{\sc Expand}}
\newcommand{\Simulate}{{\sc Simulate}}

\begin{algorithm2e}[h!]
  \Select$(s)$:\;
  \pushline
    Let $A'$ be the set of actions $a \in \cA(s)$ maximizing $\hQ^{\mathit{IM}}(s,a) + C \sqrt{\frac{\ln n_s}{n_{s,a}}}$ \; \label{alg:imselect}
    {\bf return} $a' \sim ${\sc Uniform}$(A')$ \;
  \popline
  \;
  \Update$(s,r)$:\;
  \pushline
    $r_s \leftarrow r_s + r$\;
    $n_s \leftarrow n_s + 1$\;
    $v^{\tau}_s \leftarrow \max_{a \in \cA(s)} v^{\tau}_{s,a}$\; \label{alg:imeval}
  \popline
  \;
  \Simulate$(s_{prev}, a_{prev}, s)$:\;
  \pushline
    \If{$s \not\in \cG$}{
      \Expand($s$)\;                                           \label{alg:imexpand}
      $r \leftarrow $\Playout($s$)\;
      \Update($s,r$)\;
      {\bf return} $r$\;
    }
    \Else{ 
      \lIf{$s \in \cZ$}{{\bf return} $\cR(s_{prev}, a_{prev}, s)$}
      $a \leftarrow $\Select$(s)$\;
      $s' \leftarrow \cT(s,a)$\;
      $r \leftarrow $\Simulate$(s,a,s')$\;
      \Update($s,r$)\;
      {\bf return} $r$\;
    }
  \popline
  \;
  \MCTS($s_0$):\;
  \pushline
    \lWhile{\mbox{time left}}{\Simulate$(-,-,s_0)$}
    {\bf return} $\argmax_{a \in \cA(s_0)} n_{s_0,a}$\; 
  \popline
  \vspace{0.3cm}
  \caption{Pseudo-code of MCTS with Implicit Minimax Backups. \label{alg}}
\end{algorithm2e}




\bibliographystyle{aaai}
\bibliography{im-mcts}


\end{document}
